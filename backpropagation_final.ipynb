{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def derivative_linear(x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def derivative_softmax(x, isTarget, idx):\n",
    "    if isTarget:\n",
    "        return softmax(x)[idx]\n",
    "    else:\n",
    "        return -(1-(softmax(x)[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_output(activation_function, target, output, net, idx = 0):\n",
    "  if activation_function == \"linear\":\n",
    "      result = (target - output) * derivative_linear(net)\n",
    "  elif activation_function == \"relu\":\n",
    "      result = (target - output) * derivative_relu(net)\n",
    "  elif activation_function == \"sigmoid\":\n",
    "      result = (target - output) * derivative_sigmoid(net)\n",
    "  elif activation_function == \"softmax\":\n",
    "      result = -derivative_softmax(net, True, idx)\n",
    "\n",
    "  return result\n",
    "\n",
    "def delta_hidden(activation_function, net, delta_output, weights, idx = 0):\n",
    "  if activation_function == \"linear\":\n",
    "        delta_output_np = np.array(delta_output)\n",
    "        weights_np = np.array(weights.T)\n",
    "        result = np.sum(-np.dot(delta_output_np, weights_np) * derivative_linear(net))\n",
    "  elif activation_function == \"relu\":\n",
    "        delta_output_np = np.array(delta_output)\n",
    "        weights_np = np.array(weights.T)\n",
    "        result = np.sum(-np.dot(delta_output_np, weights_np) * derivative_relu(net))\n",
    "  elif activation_function == \"sigmoid\":\n",
    "        delta_output_np = np.array(delta_output)\n",
    "        weights_np = np.array(weights.T)\n",
    "        result = np.sum(-np.dot(delta_output_np, weights_np) * derivative_sigmoid(net))\n",
    "  elif activation_function == \"softmax\":\n",
    "      result = -derivative_softmax(net, False, idx)\n",
    "\n",
    "  return result\n",
    "\n",
    "def calculate_delta_weight(activation_function, learning_rate, target, output, net, weights, x, isOutputLayer, idx = 0, out = 1):\n",
    "    if isOutputLayer:\n",
    "        return learning_rate * delta_output(activation_function, target, output, net, idx) * x\n",
    "    else:\n",
    "        return learning_rate * delta_hidden(activation_function, net, out, weights) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_sse(target, output):\n",
    "    e = 0.5 * np.sum((target - output) ** 2)\n",
    "    return e\n",
    "\n",
    "def loss_function_softmax(target, output):\n",
    "    e = -np.sum(target * np.log(output))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate_condition(stopped_by, max_iteration, error_threshold, iteration, error):\n",
    "    if stopped_by == \"max_iteration\":\n",
    "        return iteration >= max_iteration\n",
    "    elif stopped_by == \"error_threshold\":\n",
    "        return error <= error_threshold\n",
    "\n",
    "def forward_propagation(model, input_data, weights):\n",
    "    layers = model[\"layers\"]\n",
    "    output_layer = [input_data]\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        activation_function = layer[\"activation_function\"]\n",
    "        weight_matrix = weights[i]\n",
    "        bias = layer.get(\"bias\", 1)\n",
    "\n",
    "        input_activation = output_layer[-1]\n",
    "        # print(\"Dimensi input_activation:\", input_activation.shape)\n",
    "        # print(input_activation)\n",
    "        if len(input_activation.shape) == 1:\n",
    "            input_activation_with_bias = np.insert(input_activation, 0, bias)\n",
    "            input_activation_with_bias = input_activation_with_bias.reshape(1, -1)\n",
    "        else:\n",
    "            batch_size = input_activation.shape[0]\n",
    "            bias_vector = np.ones((batch_size, 1)) * bias\n",
    "            input_activation_with_bias = np.concatenate((bias_vector, input_activation), axis=1)\n",
    "        # print(\"Dimensi input_activation_with_bias:\", input_activation_with_bias.shape)\n",
    "        # print(input_activation_with_bias)\n",
    "        # print(\"Dimensi weight_matrix:\", weight_matrix.shape)\n",
    "        # print(weight_matrix)\n",
    "\n",
    "        output_linear_combination = np.dot(input_activation_with_bias, weight_matrix)\n",
    "\n",
    "        if activation_function == \"linear\":\n",
    "            activation_result = linear(output_linear_combination)\n",
    "        elif activation_function == \"relu\":\n",
    "            activation_result = relu(output_linear_combination)\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            activation_result = sigmoid(output_linear_combination)\n",
    "        elif activation_function == \"softmax\":\n",
    "            activation_result = softmax(output_linear_combination)\n",
    "\n",
    "        output_layer.append(activation_result)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "def back_propagation(model, input, initial_weights, target_input, learning_rate, batch_size, stopped_by, max_iteration, error_threshold):\n",
    "    epoch=0\n",
    "    error = np.inf\n",
    "\n",
    "    while not terminate_condition(stopped_by, max_iteration, error_threshold, epoch, error) :\n",
    "        epoch += 1\n",
    "\n",
    "        if error == np.inf:\n",
    "            error = 0\n",
    "\n",
    "        for batch in range(int(input.shape[0]/batch_size)):\n",
    "            input_batch = input[batch*batch_size:(batch+1)*batch_size]\n",
    "            target_batch = target_input[batch*batch_size:(batch+1)*batch_size]\n",
    "            output_forward = forward_propagation(model, input_batch, initial_weights)\n",
    "\n",
    "            delta_weight = [np.zeros(w.shape) for w in initial_weights]\n",
    "            for id_data in range(batch_size):\n",
    "                # Calculate error each output unit\n",
    "                error_k = []\n",
    "                error_k_divided = []\n",
    "\n",
    "                id = id_data + batch_size * batch\n",
    "\n",
    "                if (model[\"layers\"][-1][\"activation_function\"] == \"softmax\"):\n",
    "                    all_net = []\n",
    "                    # Iterate through each output unit\n",
    "                    for i in range(len(output_forward[-1][id_data])):\n",
    "                        input_x = output_forward[-2][id_data]\n",
    "                        input_x = np.insert(input_x, 0, 1)\n",
    "                        \n",
    "                        all_net.append(np.dot(input_x.T, initial_weights[-1][:,i]))\n",
    "                \n",
    "                \n",
    "                # Iterate through each output unit\n",
    "                for i in range(len(output_forward[-1][id_data])):\n",
    "                    activation_function = model[\"layers\"][-1][\"activation_function\"]\n",
    "                    output = output_forward[-1][id_data][i]\n",
    "                    target = target_batch[id_data][i]\n",
    "                    \n",
    "                    input_x = output_forward[-2][id_data]\n",
    "                    input_x = np.insert(input_x, 0, 1)\n",
    "                    \n",
    "                    weights = initial_weights[-1]\n",
    "                    error_k_j = []\n",
    "                    \n",
    "                    if(activation_function != \"softmax\"):\n",
    "                        net = np.dot(input_x.T, initial_weights[-1][:,i])\n",
    "\n",
    "                    for j in range(input_x.shape[0]):\n",
    "                        x = input_x[j]\n",
    "                        \n",
    "                        if (activation_function == \"softmax\"):\n",
    "                            error_k_j_one_unit = calculate_delta_weight(activation_function, learning_rate, target, output, all_net, weights, x, True, i)\n",
    "                            error_k_j_one_unit_divided = error_k_j_one_unit/(x * learning_rate)\n",
    "                            error_k_j.append(error_k_j_one_unit)\n",
    "                        else:\n",
    "                            error_k_j_one_unit = calculate_delta_weight(activation_function, learning_rate, target, output, net, weights, x, True)\n",
    "                            error_k_j_one_unit_divided = error_k_j_one_unit/(x * learning_rate)\n",
    "                            error_k_j.append(error_k_j_one_unit)\n",
    "                    error_k.append(error_k_j)\n",
    "                    error_k_divided.append(error_k_j_one_unit_divided)\n",
    "\n",
    "                    # if (activation_function == \"softmax\"): break\n",
    "                \n",
    "                        \n",
    "                # Calculate error each hidden unit\n",
    "                error_h = []\n",
    "                \n",
    "                # Iterate through each hidden layer\n",
    "                for j in range(len(output_forward)-2, 0, -1):\n",
    "                    error_h_j = []\n",
    "                    error_h_j_divided = []\n",
    "                    activation_function = model[\"layers\"][j - 1][\"activation_function\"]\n",
    "\n",
    "                    # Iterate through each hidden layer unit\n",
    "                    for k in range(len(output_forward[j][id_data])):\n",
    "                        output = output_forward[j][id_data][k]\n",
    "                        target = target_batch[id_data][k]\n",
    "                        \n",
    "                        input_x = output_forward[j-1][id_data]\n",
    "                        input_x = np.insert(input_x, 0, 1)\n",
    "                        \n",
    "                        net = np.dot(input_x.T, initial_weights[j - 1][:,k])\n",
    "\n",
    "                        weights = initial_weights[j - 1]\n",
    "\n",
    "                        error_h_j_k = []\n",
    "                        for l in range(input_x.shape[0]):\n",
    "                            x = input_x[l]\n",
    "                            \n",
    "                            if (activation_function == \"softmax\"):\n",
    "                                error_h_j_k_one_unit = calculate_delta_weight(activation_function, learning_rate, target, output, all_net, weights, x, False)\n",
    "                                error_h_j_k_one_unit_divided = error_h_j_k_one_unit/(x * learning_rate)\n",
    "                                error_h_j_k.append(error_h_j_k_one_unit)\n",
    "                            else:\n",
    "                                if (len(error_h) == 0):\n",
    "                                    error_h_j_k_one_unit =calculate_delta_weight(activation_function, learning_rate, target, output, net, weights, x, False, error_k_divided)\n",
    "                                    error_h_j_k_one_unit_divided = error_h_j_k_one_unit/(x * learning_rate)\n",
    "                                    error_h_j_k.append(error_h_j_k_one_unit)\n",
    "                                else:\n",
    "                                    error_h_j_k_one_unit = calculate_delta_weight(activation_function, learning_rate, target, output, net, weights, x, False, error_h_j_divided[-1])\n",
    "                                    error_h_j_k_one_unit_divided = error_h_j_k_one_unit/(x * learning_rate)\n",
    "                                    error_h_j_k.append(error_h_j_k_one_unit)\n",
    "\n",
    "                        error_h_j.append(error_h_j_k)\n",
    "                        error_h_j_divided.append(error_h_j_k_one_unit_divided)\n",
    "                    error_h.append(error_h_j)\n",
    "                    \n",
    "\n",
    "                # Update delta weight\n",
    "                for j in range(len(initial_weights)-1, -1, -1):\n",
    "                    if j == len(initial_weights)-1:\n",
    "                        for k in range(len(initial_weights[j])):\n",
    "                            for l in range(len(initial_weights[j][k])):              \n",
    "                                delta_weight[j][k][l] += error_k[l][k]\n",
    "                    else:\n",
    "                        for k in range(len(initial_weights[j])):\n",
    "                            for l in range(len(initial_weights[j][k])):\n",
    "                                delta_weight[j][k][l] += error_h[j][l][k]\n",
    "                \n",
    "            # Update weight per batch\n",
    "            # sum delta weight\n",
    "            for i in range(len(initial_weights)):\n",
    "                for j in range(len(initial_weights[i])):\n",
    "                    for k in range(len(initial_weights[i][j])):\n",
    "                        initial_weights[i][j][k] += delta_weight[i][j][k]\n",
    "            \n",
    "        # Calculate error\n",
    "        for data in range(input.shape[0]):\n",
    "            output_forward = forward_propagation(model, input, initial_weights)\n",
    "\n",
    "            activation_function = model[\"layers\"][-1][\"activation_function\"]\n",
    "            if activation_function == \"softmax\":\n",
    "                error += loss_function_softmax(target_input[data], output_forward[-1][data])\n",
    "            else:\n",
    "                error += loss_function_sse(target_input[data], output_forward[-1][data])\n",
    "\n",
    "    return initial_weights\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.1012,  0.3006,  0.1991],\n",
      "       [ 0.4024,  0.201 , -0.7019],\n",
      "       [ 0.1018, -0.799 ,  0.4987]])]\n",
      "Error:  0.001600000000000018\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/linear_small_lr.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "error = 0\n",
    "for i in range(len(new_weight)):\n",
    "    for j in range(len(new_weight[i])):\n",
    "        for k in range(len(new_weight[i][j])):\n",
    "            error += abs(new_weight[i][j][k] - final_weights[i][j][k])\n",
    "print(new_weight)\n",
    "print(\"Error: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.166,  0.338,  0.153],\n",
      "       [ 0.502,  0.226, -0.789],\n",
      "       [ 0.214, -0.718,  0.427]])]\n",
      "Error:  3.3306690738754696e-16\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/linear_two_iteration.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "error = 0\n",
    "for i in range(len(new_weight)):\n",
    "    for j in range(len(new_weight[i])):\n",
    "        for k in range(len(new_weight[i][j])):\n",
    "            error += abs(new_weight[i][j][k] - final_weights[i][j][k])\n",
    "print(new_weight)\n",
    "print(\"Error: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.22,  0.36,  0.11],\n",
      "       [ 0.64,  0.3 , -0.89],\n",
      "       [ 0.28, -0.7 ,  0.37]])]\n",
      "Error:  5.551115123125783e-17\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/linear.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "error = 0\n",
    "for i in range(len(new_weight)):\n",
    "    for j in range(len(new_weight[i])):\n",
    "        for k in range(len(new_weight[i][j])):\n",
    "            error += abs(new_weight[i][j][k] - final_weights[i][j][k])\n",
    "print(new_weight)\n",
    "print(\"Error: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.211 ,  0.105 ,  0.885 ],\n",
      "       [ 0.3033,  0.5285,  0.3005],\n",
      "       [-0.489 , -0.905 ,  0.291 ]])]\n",
      "Error:  8.326672684688674e-17\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/relu_b.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "error = 0\n",
    "for i in range(len(new_weight)):\n",
    "    for j in range(len(new_weight[i])):\n",
    "        for k in range(len(new_weight[i][j])):\n",
    "            error += abs(new_weight[i][j][k] - final_weights[i][j][k])\n",
    "print(new_weight)\n",
    "print(\"Error: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.23291176, 0.06015346],\n",
      "       [0.12884088, 0.64849474],\n",
      "       [0.837615  , 0.23158199]])]\n",
      "Error:  0.0002978285953069218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\altha\\AppData\\Local\\Temp/ipykernel_16856/4051348221.py:103: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  error_k_j_one_unit_divided = error_k_j_one_unit/(x * learning_rate)\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/sigmoid.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "error = 0\n",
    "for i in range(len(new_weight)):\n",
    "    for j in range(len(new_weight[i])):\n",
    "        for k in range(len(new_weight[i][j])):\n",
    "            error += abs(new_weight[i][j][k] - final_weights[i][j][k])\n",
    "print(new_weight)\n",
    "print(\"Error: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.00742189,  0.83363136, -0.22620947],\n",
      "       [-0.11691988,  0.83776353,  0.33315635],\n",
      "       [ 0.3723289 , -0.67004192,  0.07671302],\n",
      "       [ 0.42736295,  0.61379232, -0.33115527],\n",
      "       [ 0.3777683 ,  0.41675988,  0.53747181],\n",
      "       [-0.77750271,  0.2954632 ,  0.57403951],\n",
      "       [-0.70395444, -0.30833046,  0.4552849 ],\n",
      "       [ 0.55357479,  0.04777754, -0.61835233],\n",
      "       [ 0.70642187, -0.21360665, -0.14181521]])]\n",
      "Error:  3.344287527779199\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/softmax.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "error = 0\n",
    "for i in range(len(new_weight)):\n",
    "    for j in range(len(new_weight[i])):\n",
    "        for k in range(len(new_weight[i][j])):\n",
    "            error += abs(new_weight[i][j][k] - final_weights[i][j][k])\n",
    "print(new_weight)\n",
    "print(\"Error: \", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "['Iris-setosa' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-virginica']\n",
      "Actual values:\n",
      "0          Iris-setosa\n",
      "1       Iris-virginica\n",
      "2          Iris-setosa\n",
      "3          Iris-setosa\n",
      "4       Iris-virginica\n",
      "            ...       \n",
      "145     Iris-virginica\n",
      "146    Iris-versicolor\n",
      "147    Iris-versicolor\n",
      "148    Iris-versicolor\n",
      "149     Iris-virginica\n",
      "Name: Species, Length: 150, dtype: object\n",
      "Accuracy: 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"test/backpropagation/iris.csv\")\n",
    "dataset = dataset.drop(columns=[\"Id\"])\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "learning_data = dataset.drop(columns=[\"Species\"])\n",
    "target_data = dataset[\"Species\"]\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,9,3), max_iter=1000, activation='logistic', learning_rate='constant', learning_rate_init=0.01,)\n",
    "mlp.fit(learning_data, target_data)\n",
    "\n",
    "y_pred = mlp.predict(learning_data)\n",
    "print(\"Predicted values:\")\n",
    "print(y_pred)\n",
    "print(\"Actual values:\")\n",
    "print(target_data)\n",
    "print(\"Accuracy:\", mlp.score(learning_data, target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weight:  [array([[ 0.20231173,  0.09499589, -0.15178272],\n",
      "       [ 0.15688046, -0.0423218 , -0.56151539],\n",
      "       [ 0.69338222, -0.39335846, -0.49874664],\n",
      "       [-1.06830723,  0.25029815,  0.80246466],\n",
      "       [-0.41899993, -0.03777484,  0.65787204]])]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[1, 0, 0] [1 0 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 1, 0] [0 1 0]\n",
      "[0, 0, 1] [0 1 0]\n",
      "[0, 0, 1] [0 0 1]\n",
      "Accuracy:  0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/iris.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "# iris with implemented code\n",
    "input_data = learning_data.to_numpy()\n",
    "target = []\n",
    "for i in range(len(target_data)):\n",
    "    if target_data[i] == \"Iris-setosa\":\n",
    "        target.append([1,0,0])\n",
    "    elif target_data[i] == \"Iris-versicolor\":\n",
    "        target.append([0,1,0])\n",
    "    else:\n",
    "        target.append([0,0,1])\n",
    "target = np.array(target)\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "print(\"new weight: \", new_weight)\n",
    "output = forward_propagation(model, input_data, initial_weights)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(output[-1])):\n",
    "    max = -99999999\n",
    "    for j in range(3):\n",
    "        if output[-1][i][j] > max:\n",
    "            max = output[-1][i][j]\n",
    "            idx = j\n",
    "    \n",
    "    if (idx == 0) : \n",
    "        temp = [1,0,0]\n",
    "    elif (idx == 1):\n",
    "        temp = [0,1,0]\n",
    "    else:\n",
    "        temp = [0,0,1]\n",
    "\n",
    "    print(temp,target[i])\n",
    "    if (np.array_equal(temp, target[i])) :\n",
    "        count += 1\n",
    "print(\"Accuracy: \", count/len(output[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model\n",
    "def save_model(model, input_data, new_weight, stopped_by):\n",
    "    data = {\n",
    "        \"case\": {\n",
    "            \"model\": model,\n",
    "            \"input\": input_data.tolist(),\n",
    "            \"weights\": new_weight[0].tolist()  # Access the NumPy array inside the list\n",
    "        },\n",
    "        \"expect\": {\n",
    "            \"stopped_by\": stopped_by,\n",
    "            \"final_weights\": new_weight[0].tolist()  # Access the NumPy array inside the list\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(\"test/model.json\", \"w\") as file:\n",
    "        json.dump(data, file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
