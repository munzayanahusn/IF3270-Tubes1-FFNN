{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def derivative_linear(x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def derivative_softmax(x, isTarget):\n",
    "    if isTarget:\n",
    "        return softmax(x)\n",
    "    else:\n",
    "        return -(1-softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_output(activation_function, target, output, net):\n",
    "  if activation_function == \"linear\":\n",
    "      result = (target - output) * derivative_linear(net)\n",
    "  elif activation_function == \"relu\":\n",
    "      result = (target - output) * derivative_relu(net)\n",
    "  elif activation_function == \"sigmoid\":\n",
    "      result = (target - output) * derivative_sigmoid(net)\n",
    "  elif activation_function == \"softmax\":\n",
    "      result = - derivative_softmax(net, True)\n",
    "\n",
    "  return result\n",
    "\n",
    "def delta_hidden(activation_function, net, delta_output, weights):\n",
    "  if activation_function == \"linear\":\n",
    "      result = np.dot(delta_output, weights.T) * derivative_linear(net)\n",
    "  elif activation_function == \"relu\":\n",
    "      result = np.dot(delta_output, weights.T) * derivative_relu(net)\n",
    "  elif activation_function == \"sigmoid\":\n",
    "      result = np.dot(delta_output, weights.T) * derivative_sigmoid(net)\n",
    "  elif activation_function == \"softmax\":\n",
    "      result = - derivative_softmax(net, False)\n",
    "\n",
    "  return result\n",
    "\n",
    "def delta_error(activation_function, target, output, net, weights, x, isOutputLayer):\n",
    "    if isOutputLayer:\n",
    "        return - delta_output(activation_function, target, output, net) * x\n",
    "    else:\n",
    "        return - delta_hidden(activation_function, net, delta_output, weights) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_sse(target, output):\n",
    "    e = np.sum(np.sum(np.square(output - target), axis=1) / 2, axis=0)\n",
    "    return e\n",
    "\n",
    "def loss_function_softmax(target, output):\n",
    "    e = np.sum(-np.sum(target * np.log(output), axis=1), axis=0)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate_condition(stopped_by, max_iteration, error_threshold, iteration, error):\n",
    "    if stopped_by == \"max_iteration\":\n",
    "        return iteration >= max_iteration\n",
    "    elif stopped_by == \"error_threshold\":\n",
    "        return error <= error_threshold\n",
    "\n",
    "def forward_propagation(model, input_data, weights):\n",
    "    layers = model[\"layers\"]\n",
    "    output_layer = [input_data]\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        activation_function = layer[\"activation_function\"]\n",
    "        weight_matrix = weights[i]\n",
    "        bias = layer.get(\"bias\", 1)\n",
    "\n",
    "        input_activation = output_layer[-1]\n",
    "        # print(\"Dimensi input_activation:\", input_activation.shape)\n",
    "        # print(input_activation)\n",
    "        if len(input_activation.shape) == 1:\n",
    "            input_activation_with_bias = np.insert(input_activation, 0, bias)\n",
    "            input_activation_with_bias = input_activation_with_bias.reshape(1, -1)\n",
    "        else:\n",
    "            batch_size = input_activation.shape[0]\n",
    "            bias_vector = np.ones((batch_size, 1)) * bias\n",
    "            input_activation_with_bias = np.concatenate((bias_vector, input_activation), axis=1)\n",
    "        # print(\"Dimensi input_activation_with_bias:\", input_activation_with_bias.shape)\n",
    "        # print(input_activation_with_bias)\n",
    "        # print(\"Dimensi weight_matrix:\", weight_matrix.shape)\n",
    "        # print(weight_matrix)\n",
    "\n",
    "        output_linear_combination = np.dot(input_activation_with_bias, weight_matrix)\n",
    "\n",
    "        if activation_function == \"linear\":\n",
    "            activation_result = linear(output_linear_combination)\n",
    "        elif activation_function == \"relu\":\n",
    "            activation_result = relu(output_linear_combination)\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            activation_result = sigmoid(output_linear_combination)\n",
    "        elif activation_function == \"softmax\":\n",
    "            activation_result = softmax(output_linear_combination)\n",
    "\n",
    "        output_layer.append(activation_result)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "def back_propagation(model, input, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold):\n",
    "\n",
    "    i=0\n",
    "    error = np.inf\n",
    "\n",
    "    while not terminate_condition(stopped_by, max_iteration, error_threshold, i, error):\n",
    "        i += 1\n",
    "        for batch in range(int(input.shape[0]/batch_size)):\n",
    "            input_batch = input[batch*batch_size:(batch+1)*batch_size]\n",
    "            target_batch = target[batch*batch_size:(batch+1)*batch_size]\n",
    "            output_forward = forward_propagation(model, input_batch, initial_weights)\n",
    "            delta_weight = [np.zeros(w.shape) for w in initial_weights]\n",
    "            for id_data in range(batch_size):\n",
    "                # Calculate error each output unit\n",
    "                error_k = []\n",
    "                for i in range(len(output_forward[-1])):\n",
    "                    # masih sigmoid\n",
    "                    activation_function = model[\"layers\"][-1][\"activation_function\"]\n",
    "                    output = output_forward[-1][i]\n",
    "                    target = target_batch[id_data][i]\n",
    "                    print(\"output:\", output)\n",
    "                    print(\"initial_weights:\", initial_weights[-1][:,i])\n",
    "                    net = np.dot(output_forward[-2], initial_weights[-1][:,i])\n",
    "                    weights = initial_weights[-1]\n",
    "                    x = output_forward[-2][i]\n",
    "                    error_k.append(delta_error(activation_function, target, output, net, weights, x, True))\n",
    "\n",
    "                # Calculate error each hidden unit\n",
    "                error_h = []\n",
    "                # for each hidden layer\n",
    "                for j in range(len(output_forward)-2, 0, -1):\n",
    "                    error_h_j = []\n",
    "                    for k in range(len(output_forward[j])):\n",
    "                        # masih sigmoid\n",
    "                        activation_function = model[\"layers\"][j][\"activation_function\"]\n",
    "                        output = output_forward[j][k]\n",
    "                        target = target_batch[id_data][k]\n",
    "                        net = np.dot(output_forward[j-1], initial_weights[j][:,k])\n",
    "                        weights = initial_weights[j]\n",
    "                        x = output_forward[j-1][k]\n",
    "                        error_h_j.append(delta_error(activation_function, target, output, net, weights, x, False))\n",
    "                    error_h.append(error_h_j)\n",
    "\n",
    "                # Update delta weight\n",
    "                for j in range(len(initial_weights)):\n",
    "                    if j == len(initial_weights)-1:\n",
    "                        for k in range(len(initial_weights[j])):\n",
    "                            delta_weight[j][:,k] += error_k[k]\n",
    "                    else:\n",
    "                        for k in range(len(initial_weights[j])):\n",
    "                           delta_weight[j][:,k] += error_h[j][k] \n",
    "\n",
    "            # Update weight per batch\n",
    "            initial_weights += learning_rate * delta_weight\n",
    "\n",
    "        # Calculate error\n",
    "        for data in range(input.shape[0]):\n",
    "            output_forward = forward_propagation(model, input[data], initial_weights)\n",
    "            activation_function = model[\"layers\"][-1][\"activation_function\"]\n",
    "            if activation_function == \"softmax\":\n",
    "                error += loss_function_softmax(target[data], output_forward[-1])\n",
    "            else:\n",
    "                error += loss_function_sse(target[data], output_forward[-1])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights [array([[ 0.1, -0.1,  0.1, -0.1],\n",
      "       [-0.1,  0.1, -0.1,  0.1],\n",
      "       [ 0.1,  0.1, -0.1, -0.1]]), array([[ 0.12, -0.1 ],\n",
      "       [-0.12,  0.1 ],\n",
      "       [ 0.12, -0.1 ],\n",
      "       [-0.12,  0.1 ],\n",
      "       [ 0.02,  0.  ]])]\n",
      "output: [0.58685036 0.41314964]\n",
      "initial_weights: [ 0.12 -0.12  0.12 -0.12  0.02]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,4) and (5,) not aligned: 4 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m final_weights \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39marray(layer_weights) \u001b[38;5;28;01mfor\u001b[39;00m layer_weights \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpect\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial Weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, initial_weights)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mback_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopped_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 68\u001b[0m, in \u001b[0;36mback_propagation\u001b[0;34m(model, input, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, initial_weights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:,i])\n\u001b[0;32m---> 68\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_forward\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m weights \u001b[38;5;241m=\u001b[39m initial_weights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m output_forward[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][i]\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,4) and (5,) not aligned: 4 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/softmax_two_layer.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "print(\"Initial Weights\", initial_weights)\n",
    "\n",
    "back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 3.99,  2.96],\n",
      "       [-0.71,  2.8 ],\n",
      "       [-2.43, -0.2 ],\n",
      "       [-1.9 ,  2.62],\n",
      "       [-2.58,  1.43],\n",
      "       [-3.43, -0.25],\n",
      "       [ 1.15, -2.3 ],\n",
      "       [ 4.28,  3.45]]), array([[0.   , 0.595, 0.   , 0.003],\n",
      "       [0.451, 0.109, 0.   , 0.   ],\n",
      "       [0.323, 0.   , 0.363, 0.   ],\n",
      "       [0.552, 0.   , 0.028, 0.   ],\n",
      "       [0.501, 0.   , 0.215, 0.   ],\n",
      "       [0.418, 0.   , 0.468, 0.   ],\n",
      "       [0.   , 0.   , 0.215, 0.245],\n",
      "       [0.017, 0.673, 0.   , 0.   ]]), array([[0.58685036, 0.41314964],\n",
      "       [0.53612693, 0.46387307],\n",
      "       [0.51726314, 0.48273686],\n",
      "       [0.52308358, 0.47691642],\n",
      "       [0.51561492, 0.48438508],\n",
      "       [0.50626967, 0.49373033],\n",
      "       [0.54428366, 0.45571634],\n",
      "       [0.59008578, 0.40991422]])]\n"
     ]
    }
   ],
   "source": [
    "output_layer = forward_propagation(model, input_data, initial_weights)\n",
    "print(output_layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
