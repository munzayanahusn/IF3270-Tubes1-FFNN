{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def derivative_linear(x):\n",
    "    return np.ones(x.shape)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def derivative_softmax(x, isTarget):\n",
    "    if isTarget:\n",
    "        return softmax(x)\n",
    "    else:\n",
    "        return -(1-softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_output(activation_function, target, output, net):\n",
    "  if activation_function == \"linear\":\n",
    "      result = (target - output) * derivative_linear(net)\n",
    "  elif activation_function == \"relu\":\n",
    "      result = (target - output) * derivative_relu(net)\n",
    "  elif activation_function == \"sigmoid\":\n",
    "      result = (target - output) * derivative_sigmoid(net)\n",
    "  elif activation_function == \"softmax\":\n",
    "      result = -derivative_softmax(net, True)\n",
    "\n",
    "  return result\n",
    "\n",
    "def delta_hidden(activation_function, net, delta_output, weights):\n",
    "  if activation_function == \"linear\":\n",
    "      result = np.dot(delta_output, weights.T) * derivative_linear(net)\n",
    "  elif activation_function == \"relu\":\n",
    "      result = np.dot(delta_output, weights.T) * derivative_relu(net)\n",
    "  elif activation_function == \"sigmoid\":\n",
    "      result = np.dot(delta_output, weights.T) * derivative_sigmoid(net)\n",
    "  elif activation_function == \"softmax\":\n",
    "      result = -derivative_softmax(net, False)\n",
    "\n",
    "  return result\n",
    "\n",
    "def calculate_delta_weight(activation_function, learning_rate, target, output, net, weights, x, isOutputLayer):\n",
    "    if isOutputLayer:\n",
    "        return learning_rate * delta_output(activation_function, target, output, net) * x\n",
    "    else:\n",
    "        return learning_rate * delta_hidden(activation_function, net, delta_output, weights) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_sse(target, output):\n",
    "    # print(\"loss function sse target: \", target)\n",
    "    # print(\"loss function sse output: \", output)\n",
    "    \n",
    "    e = 0.5 * np.sum((target - output) ** 2)\n",
    "    return e\n",
    "\n",
    "def loss_function_softmax(target, output):\n",
    "    e = -np.sum(target * np.log(output))\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminate_condition(stopped_by, max_iteration, error_threshold, iteration, error):\n",
    "    if stopped_by == \"max_iteration\":\n",
    "        return iteration >= max_iteration\n",
    "    elif stopped_by == \"error_threshold\":\n",
    "        return error <= error_threshold\n",
    "\n",
    "def forward_propagation(model, input_data, weights):\n",
    "    layers = model[\"layers\"]\n",
    "    output_layer = [input_data]\n",
    "\n",
    "    for i, layer in enumerate(layers):\n",
    "        activation_function = layer[\"activation_function\"]\n",
    "        weight_matrix = weights[i]\n",
    "        bias = layer.get(\"bias\", 1)\n",
    "\n",
    "        input_activation = output_layer[-1]\n",
    "        # print(\"Dimensi input_activation:\", input_activation.shape)\n",
    "        # print(input_activation)\n",
    "        if len(input_activation.shape) == 1:\n",
    "            input_activation_with_bias = np.insert(input_activation, 0, bias)\n",
    "            input_activation_with_bias = input_activation_with_bias.reshape(1, -1)\n",
    "        else:\n",
    "            batch_size = input_activation.shape[0]\n",
    "            bias_vector = np.ones((batch_size, 1)) * bias\n",
    "            input_activation_with_bias = np.concatenate((bias_vector, input_activation), axis=1)\n",
    "        # print(\"Dimensi input_activation_with_bias:\", input_activation_with_bias.shape)\n",
    "        # print(input_activation_with_bias)\n",
    "        # print(\"Dimensi weight_matrix:\", weight_matrix.shape)\n",
    "        # print(weight_matrix)\n",
    "\n",
    "        output_linear_combination = np.dot(input_activation_with_bias, weight_matrix)\n",
    "\n",
    "        if activation_function == \"linear\":\n",
    "            activation_result = linear(output_linear_combination)\n",
    "        elif activation_function == \"relu\":\n",
    "            activation_result = relu(output_linear_combination)\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            activation_result = sigmoid(output_linear_combination)\n",
    "        elif activation_function == \"softmax\":\n",
    "            activation_result = softmax(output_linear_combination)\n",
    "\n",
    "        output_layer.append(activation_result)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "def back_propagation(model, input, initial_weights, target_input, learning_rate, batch_size, stopped_by, max_iteration, error_threshold):\n",
    "\n",
    "    epoch=0\n",
    "    error = np.inf\n",
    "\n",
    "    while not terminate_condition(stopped_by, max_iteration, error_threshold, epoch, error) :\n",
    "        epoch += 1\n",
    "\n",
    "        if error == np.inf:\n",
    "            error = 0\n",
    "\n",
    "        for batch in range(int(input.shape[0]/batch_size)):\n",
    "            input_batch = input[batch*batch_size:(batch+1)*batch_size]\n",
    "            target_batch = target_input[batch*batch_size:(batch+1)*batch_size]\n",
    "            output_forward = forward_propagation(model, input_batch, initial_weights)\n",
    "            \n",
    "            delta_weight = [np.zeros(w.shape) for w in initial_weights]\n",
    "            for id_data in range(batch_size):\n",
    "                # Calculate error each output unit\n",
    "                error_k = []\n",
    "                id = id_data + batch_size * batch\n",
    "\n",
    "                '''\n",
    "                if (model[\"layers\"][-1][\"activation_function\"] == \"softmax\"):\n",
    "                    input_x = np.insert(output_forward[-2], 0, 1)\n",
    "                    all_net = np.dot(input_x, initial_weights[-1].T)\n",
    "                '''\n",
    "                \n",
    "                # Iterate through each output unit\n",
    "                for i in range(len(output_forward[-1][id_data])):\n",
    "                    activation_function = model[\"layers\"][-1][\"activation_function\"]\n",
    "                    output = output_forward[-1][id_data][i]\n",
    "                    target = target_batch[id_data][i]\n",
    "\n",
    "                    # print(\"output:\", output) \n",
    "                    # print(\"output_forward :\", output_forward)\n",
    "                    # print(\"initial_weights:\", initial_weights[-1][:,i])\n",
    "                    \n",
    "                    input_x = output_forward[-2][id_data]\n",
    "                    input_x = np.insert(input_x, 0, 1)\n",
    "                    # print(\"input x:\", input_x)\n",
    "                    \n",
    "                    net = np.dot(input_x.T, initial_weights[-1][:,i])\n",
    "\n",
    "                    weights = initial_weights[-1]\n",
    "\n",
    "                    error_k_j = []\n",
    "                    for j in range(input_x.shape[0]):\n",
    "                        x = input_x[j]\n",
    "                        \n",
    "                        if (activation_function == \"softmax\"):\n",
    "                            error_k_j.append(calculate_delta_weight(activation_function, learning_rate, target, output, all_net, weights, x, True))\n",
    "                        else:\n",
    "                            error_k_j.append(calculate_delta_weight(activation_function, learning_rate, target, output, net, weights, x, True))\n",
    "\n",
    "                    error_k.append(error_k_j)\n",
    "                        \n",
    "                # print(\"error_k Inside:\", error_k)\n",
    "\n",
    "                # Calculate error each hidden unit\n",
    "                error_h = []\n",
    "                \n",
    "                # Iterate through each hidden layer\n",
    "                for j in range(len(output_forward)-2, 1, -1):\n",
    "                    error_h_j = []\n",
    "                    activation_function = model[\"layers\"][j][\"activation_function\"]\n",
    "\n",
    "                    # Iterate through each hidden layer unit\n",
    "                    for k in range(len(output_forward[j][id_data])):\n",
    "                        output = output_forward[j][id_data][k]\n",
    "                        target = target_batch[id_data][k]\n",
    "\n",
    "                        # print(\"output:\", output) \n",
    "                        # print(\"output_forward :\", output_forward)\n",
    "                        # print(\"initial_weights:\", initial_weights[j][:,k])\n",
    "                        \n",
    "                        input_x = output_forward[j-1][id_data]\n",
    "                        input_x = np.insert(input_x, 0, 1)\n",
    "                        # print(\"input x:\", input_x)\n",
    "                        \n",
    "                        net = np.dot(input_x.T, initial_weights[j][:,k])\n",
    "\n",
    "                        weights = initial_weights[j]\n",
    "\n",
    "                        error_h_j_k = []\n",
    "                        for l in range(input_x.shape[0]):\n",
    "                            x = input_x[l]\n",
    "                            \n",
    "                            if (activation_function == \"softmax\"):\n",
    "                                error_h_j_k.append(calculate_delta_weight(activation_function, learning_rate, target, output, all_net, weights, x, False))\n",
    "                            else:\n",
    "                                error_h_j_k.append(calculate_delta_weight(activation_function, learning_rate, target, output, net, weights, x, False))\n",
    "\n",
    "                        error_h_j.append(error_h_j_k)\n",
    "                    error_h.append(error_h_j)\n",
    "\n",
    "                # Update delta weight\n",
    "                # print (\"error_k:\", error_k)\n",
    "                # print (\"delta_weight:\", delta_weight)\n",
    "\n",
    "                for j in range(len(initial_weights)-1, -1, -1):\n",
    "                    if j == len(initial_weights)-1:\n",
    "                        for k in range(len(initial_weights[j])):\n",
    "                            for l in range(len(initial_weights[j][k])):\n",
    "                                # print(\"j:\", j)\n",
    "                                # print(\"k:\", k)\n",
    "                                # print(\"l:\", l)\n",
    "                                # print(\"error_k[l][k]:\", error_k[l][k])\n",
    "                                # print(\"delta_weight[j][k][l]:\", delta_weight[j][k][l])\n",
    "                                \n",
    "                                delta_weight[j][k][l] += error_k[l][k]\n",
    "                    else:\n",
    "                        for k in range(len(initial_weights[j])):\n",
    "                            for l in range(len(initial_weights[j][k])):\n",
    "                                delta_weight[j][k][l] += error_h[j][k][l]\n",
    "\n",
    "            # Update weight per batch\n",
    "            # sum delta weight\n",
    "            for i in range(len(initial_weights)):\n",
    "                for j in range(len(initial_weights[i])):\n",
    "                    for k in range(len(initial_weights[i][j])):\n",
    "                        initial_weights[i][j][k] += delta_weight[i][j][k]\n",
    "\n",
    "            # print(\" Weights: \", initial_weights)\n",
    "            # print(\"Delta Weights: \", delta_weight)\n",
    "\n",
    "        # Calculate error\n",
    "        for data in range(input.shape[0]):\n",
    "            # print(\"Data: \", data)\n",
    "            # print(\"target_input\", target_input)\n",
    "            # print(\"input_data\", input)\n",
    "            \n",
    "            output_forward = forward_propagation(model, input, initial_weights)\n",
    "            # print(\"output_forward\", output_forward)\n",
    "\n",
    "            activation_function = model[\"layers\"][-1][\"activation_function\"]\n",
    "            if activation_function == \"softmax\":\n",
    "                error += loss_function_softmax(target_input[data], output_forward[-1][data])\n",
    "            else:\n",
    "                error += loss_function_sse(target_input[data], output_forward[-1][data])\n",
    "\n",
    "    # print(\"Final: \", initial_weights)\n",
    "    return initial_weights\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weight:  [array([[-0.36946807, -0.36706698, -0.68519825],\n",
      "       [ 1.5506238 , -1.14787811, -0.93440806],\n",
      "       [-1.15069189,  1.55188113, -0.93141998],\n",
      "       [-0.74939998, -0.75107   ,  1.20062979],\n",
      "       [ 0.01      ,  0.01      ,  0.01      ]])]\n",
      "output:  [[0.76515554 0.18020709 0.16525917]\n",
      " [0.17943797 0.7658123  0.16567179]\n",
      " [0.24622131 0.24635702 0.62607889]\n",
      " [0.76515554 0.18020709 0.16525917]\n",
      " [0.17943797 0.7658123  0.16567179]]\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/iris_1.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "input_data = np.array(data[\"case\"][\"input\"])\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "target = np.array(data[\"case\"][\"target\"])\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "print(\"new weight: \", new_weight)\n",
    "output = forward_propagation(model, input_data, new_weight)\n",
    "print(\"output: \", output[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_layer = forward_propagation(model, input_data, initial_weights)\n",
    "# print(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation with sklearn\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:\n",
      "['Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-setosa' 'Iris-virginica']\n",
      "Actual values:\n",
      "0      Iris-versicolor\n",
      "1      Iris-versicolor\n",
      "2      Iris-versicolor\n",
      "3      Iris-versicolor\n",
      "4      Iris-versicolor\n",
      "            ...       \n",
      "145        Iris-setosa\n",
      "146        Iris-setosa\n",
      "147     Iris-virginica\n",
      "148        Iris-setosa\n",
      "149     Iris-virginica\n",
      "Name: Species, Length: 150, dtype: object\n",
      "Accuracy: 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv(\"test/backpropagation/iris.csv\")\n",
    "dataset = dataset.drop(columns=[\"Id\"])\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)  # Shuffling the dataset\n",
    "learning_data = dataset.drop(columns=[\"Species\"])\n",
    "target_data = dataset[\"Species\"]\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,9,3), max_iter=1000, activation='logistic', learning_rate='constant', learning_rate_init=0.01,)\n",
    "mlp.fit(learning_data, target_data)\n",
    "\n",
    "y_pred = mlp.predict(learning_data)\n",
    "print(\"Predicted values:\")\n",
    "print(y_pred)\n",
    "print(\"Actual values:\")\n",
    "print(target_data)\n",
    "print(\"Accuracy:\", mlp.score(learning_data, target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new weight:  [array([[ 0.30183608, -0.09826237, -0.11831176],\n",
      "       [ 0.22773577, -0.91320266, -0.99410488],\n",
      "       [ 1.30754304, -0.5886788 , -0.58624191],\n",
      "       [-2.22100868, -0.25857407, -0.28975625],\n",
      "       [-0.93139097,  0.0025303 ,  0.0309893 ]])]\n",
      "output:  [[4.90912823e-04 2.07205895e-04 1.12328769e-04]\n",
      " [2.63197005e-03 3.50646590e-04 2.02894190e-04]\n",
      " [7.50645754e-03 3.21595635e-04 1.79710329e-04]\n",
      " [9.69158032e-04 9.40748784e-05 4.85125437e-05]\n",
      " [4.73105715e-03 1.20273758e-04 6.31665691e-05]\n",
      " [1.22457343e-04 1.36828141e-05 6.19401417e-06]\n",
      " [6.83128010e-03 3.50197646e-04 1.97379105e-04]\n",
      " [2.53813089e-04 2.34975693e-04 1.32475548e-04]\n",
      " [3.55670686e-04 8.94696099e-05 4.66403757e-05]\n",
      " [7.84152146e-03 4.57374837e-04 2.62010214e-04]\n",
      " [6.75168328e-05 8.79021980e-05 4.64617325e-05]\n",
      " [9.74643827e-01 2.53332780e-04 1.52771773e-04]\n",
      " [9.22924069e-01 4.24587466e-04 2.60405138e-04]\n",
      " [9.06631845e-01 6.00040228e-04 3.69888396e-04]\n",
      " [8.22994726e-01 7.94862096e-04 5.00233136e-04]\n",
      " [2.05313073e-04 9.18842420e-05 4.90483139e-05]\n",
      " [5.11433547e-03 3.69002646e-04 2.03282837e-04]\n",
      " [1.43983308e-04 6.59263325e-05 3.40532797e-05]\n",
      " [9.45407897e-01 5.02695389e-04 3.08336731e-04]\n",
      " [8.58155984e-01 2.05582475e-03 1.36950473e-03]\n",
      " [5.48679297e-04 2.96712468e-04 1.69141886e-04]\n",
      " [2.64746943e-03 2.15047895e-04 1.18502052e-04]\n",
      " [7.62650736e-01 2.74008420e-03 1.81922019e-03]\n",
      " [1.57294844e-03 1.01641782e-04 5.18477436e-05]\n",
      " [5.89561477e-04 1.90366753e-04 1.04707779e-04]\n",
      " [1.26757982e-04 1.19298655e-04 6.25414904e-05]\n",
      " [7.22220487e-03 2.87923243e-04 1.62735047e-04]\n",
      " [9.82038273e-01 3.16036796e-04 1.89563417e-04]\n",
      " [8.70396274e-01 7.94619847e-04 5.07402083e-04]\n",
      " [8.62887095e-01 5.62332932e-04 3.51093416e-04]\n",
      " [8.85590712e-01 1.12880334e-03 7.18008383e-04]\n",
      " [2.78085494e-04 9.92550094e-05 5.13600859e-05]\n",
      " [8.35243123e-01 1.06520257e-03 6.71735429e-04]\n",
      " [2.25176595e-04 7.55288279e-05 3.84723401e-05]\n",
      " [9.17920439e-01 1.76865363e-03 1.18262830e-03]\n",
      " [6.57414091e-04 1.48515593e-04 7.83753436e-05]\n",
      " [6.31714730e-05 4.20437843e-05 1.98994473e-05]\n",
      " [4.98843526e-05 1.52063553e-05 6.97041353e-06]\n",
      " [5.78180959e-03 3.05453421e-04 1.73094235e-04]\n",
      " [2.75428298e-03 2.92129400e-04 1.66317767e-04]\n",
      " [9.35080242e-01 2.29380082e-03 1.55097176e-03]\n",
      " [9.21269108e-01 9.40260071e-04 5.97060206e-04]\n",
      " [8.68185847e-01 1.48440770e-03 9.70255248e-04]\n",
      " [9.34063956e-01 7.63208087e-04 4.82303139e-04]\n",
      " [9.27214723e-01 6.78703648e-04 4.22904852e-04]\n",
      " [4.28584356e-03 1.25094073e-04 6.67842633e-05]\n",
      " [2.08585722e-03 1.40800525e-04 7.42598663e-05]\n",
      " [5.29659063e-04 7.21152378e-05 3.71144574e-05]\n",
      " [3.09784724e-04 1.02528571e-04 5.34980653e-05]\n",
      " [2.79394475e-03 1.22738858e-04 6.58515116e-05]\n",
      " [3.82172107e-04 1.82022373e-04 9.73910466e-05]\n",
      " [1.01555413e-03 7.43101741e-04 4.49802626e-04]\n",
      " [9.03598901e-01 1.43627588e-03 9.41937532e-04]\n",
      " [9.74649727e-01 1.25883835e-03 8.36686994e-04]\n",
      " [1.62562291e-05 2.73816755e-05 1.24499066e-05]\n",
      " [9.91791919e-05 5.17874440e-05 2.54271786e-05]\n",
      " [9.55092095e-01 5.43523996e-04 3.32605577e-04]\n",
      " [2.71313426e-03 1.80295459e-04 9.79876607e-05]\n",
      " [8.60138384e-01 1.24509263e-03 8.04897049e-04]\n",
      " [1.11116662e-03 2.70491301e-04 1.46415829e-04]\n",
      " [2.01823894e-04 4.84447059e-05 2.33335259e-05]\n",
      " [7.77580428e-05 1.02700357e-04 5.17401489e-05]\n",
      " [2.92748075e-03 1.21106420e-04 6.30135231e-05]\n",
      " [6.33133052e-03 3.32452030e-04 1.88938228e-04]\n",
      " [9.69374600e-03 3.38830031e-04 1.91614104e-04]\n",
      " [5.51180299e-03 1.03502844e-04 5.39333889e-05]\n",
      " [3.54758874e-04 2.48904894e-04 1.38313354e-04]\n",
      " [1.59294248e-04 5.82222123e-05 2.94159293e-05]\n",
      " [2.08969596e-04 6.58596454e-05 3.36337843e-05]\n",
      " [9.47742146e-03 1.17640827e-03 7.13597542e-04]\n",
      " [4.62857172e-03 1.60265080e-04 8.86714606e-05]\n",
      " [1.14682658e-02 5.59504190e-04 3.24150635e-04]\n",
      " [9.45420915e-01 7.88163139e-04 5.00817458e-04]\n",
      " [9.45900246e-04 1.75970096e-04 9.53092245e-05]\n",
      " [2.63531785e-03 4.15284471e-04 2.36847331e-04]\n",
      " [4.95402040e-05 4.12312686e-05 1.95002732e-05]\n",
      " [1.90800147e-03 1.86345293e-04 1.00936053e-04]\n",
      " [7.03740877e-03 5.86210393e-04 3.50018432e-04]\n",
      " [2.58671072e-03 9.58023867e-05 4.95967912e-05]\n",
      " [9.36556490e-01 6.07356050e-04 3.80601094e-04]\n",
      " [3.46157500e-03 6.86645483e-05 3.46045974e-05]\n",
      " [8.07588974e-01 9.35094223e-04 5.94273354e-04]\n",
      " [3.43423751e-04 1.05984991e-04 5.72289813e-05]\n",
      " [9.43801283e-01 6.23413213e-04 3.93001716e-04]\n",
      " [1.17364619e-03 1.87767978e-04 1.03408907e-04]\n",
      " [5.10913664e-03 4.88549597e-04 2.82872239e-04]\n",
      " [4.49340832e-04 1.07314375e-04 5.63724776e-05]\n",
      " [4.35735942e-03 3.76448897e-04 2.14960256e-04]\n",
      " [9.30676534e-01 6.61356153e-04 4.18012810e-04]\n",
      " [9.44220519e-01 5.50736569e-04 3.40552988e-04]\n",
      " [8.46636314e-01 1.20538293e-03 7.72753609e-04]\n",
      " [1.74928230e-04 1.76178646e-04 9.27202913e-05]\n",
      " [6.05293527e-05 2.84469434e-05 1.32980104e-05]\n",
      " [2.10947978e-02 3.90792465e-04 2.27476288e-04]\n",
      " [2.40006848e-02 4.36379695e-04 2.50426125e-04]\n",
      " [9.41208313e-01 1.05008802e-03 6.70832567e-04]\n",
      " [3.45537926e-04 1.01926726e-04 5.42968601e-05]\n",
      " [1.86721398e-04 1.23139703e-04 6.45420979e-05]\n",
      " [2.01671773e-03 1.97542152e-04 1.06368351e-04]\n",
      " [3.49979996e-04 2.85310657e-04 1.54529568e-04]\n",
      " [9.66790935e-01 4.70832023e-04 2.92396028e-04]\n",
      " [1.39558861e-04 4.09145636e-05 1.97054069e-05]\n",
      " [8.39343154e-03 5.45367993e-04 3.15872899e-04]\n",
      " [7.28590433e-04 9.78948240e-05 5.16098765e-05]\n",
      " [8.73112929e-01 1.34647726e-03 8.70908036e-04]\n",
      " [9.61750838e-04 1.67009406e-04 9.09506854e-05]\n",
      " [8.85408967e-01 1.22879779e-03 7.86121865e-04]\n",
      " [1.73773633e-03 7.57889321e-05 3.83657503e-05]\n",
      " [9.84205185e-05 9.19403138e-05 4.76715061e-05]\n",
      " [1.15486286e-04 1.19328841e-04 6.27355905e-05]\n",
      " [8.76531974e-01 8.42335075e-04 5.34698359e-04]\n",
      " [3.03739053e-04 3.14873619e-04 1.77366135e-04]\n",
      " [2.16078110e-02 1.03843551e-03 6.34271562e-04]\n",
      " [8.92356955e-01 1.34579696e-03 8.65531621e-04]\n",
      " [8.63980228e-03 2.36788565e-04 1.30687903e-04]\n",
      " [3.27802422e-04 6.66991694e-05 3.38143459e-05]\n",
      " [9.98667932e-05 6.05742592e-05 3.06610872e-05]\n",
      " [5.19178209e-03 1.88567395e-04 1.02296852e-04]\n",
      " [6.92437426e-04 1.86600565e-04 9.90190802e-05]\n",
      " [2.40125058e-02 1.07265126e-03 6.60658514e-04]\n",
      " [9.45328346e-01 8.58008076e-04 5.48338545e-04]\n",
      " [9.16149460e-01 7.88602778e-04 4.95273183e-04]\n",
      " [9.17301708e-01 1.27727647e-03 8.40411055e-04]\n",
      " [8.02482370e-03 3.23787926e-04 1.85598469e-04]\n",
      " [1.48963917e-03 1.82873827e-04 1.01580850e-04]\n",
      " [8.85590712e-01 1.12880334e-03 7.18008383e-04]\n",
      " [3.93809926e-03 5.49558767e-04 3.18049979e-04]\n",
      " [9.77031007e-01 3.50841327e-04 2.14375819e-04]\n",
      " [1.51930137e-04 7.60862770e-05 3.92929921e-05]\n",
      " [1.94737036e-03 2.38754275e-04 1.27895347e-04]\n",
      " [6.07000066e-06 2.92726067e-05 1.33338533e-05]\n",
      " [8.82390261e-01 5.69527094e-04 3.46913407e-04]\n",
      " [5.72474137e-03 2.40053152e-04 1.34643064e-04]\n",
      " [9.82455977e-05 3.15614902e-05 1.54733225e-05]\n",
      " [3.54758874e-04 2.48904894e-04 1.38313354e-04]\n",
      " [9.14383341e-01 8.63943702e-04 5.47010621e-04]\n",
      " [9.22923109e-01 1.34538703e-03 8.77931137e-04]\n",
      " [9.39573527e-01 7.63015144e-04 4.80811546e-04]\n",
      " [9.68395357e-01 4.76718597e-04 2.96613362e-04]\n",
      " [8.95940680e-01 1.98920168e-03 1.32955009e-03]\n",
      " [5.43240667e-03 1.57094325e-04 8.38539308e-05]\n",
      " [6.80155522e-04 1.66004392e-04 8.88874623e-05]\n",
      " [8.85590712e-01 1.12880334e-03 7.18008383e-04]\n",
      " [2.61389403e-03 2.99794914e-04 1.68264695e-04]\n",
      " [4.94814400e-02 9.10840212e-04 5.58871501e-04]\n",
      " [8.90973333e-01 1.01044297e-03 6.48208441e-04]\n",
      " [9.33143973e-01 7.38668955e-04 4.61604432e-04]\n",
      " [2.60856901e-04 1.36666920e-04 7.18926503e-05]\n",
      " [9.25080145e-01 3.42357100e-04 2.04299807e-04]\n",
      " [2.34803932e-05 2.73733240e-05 1.26284172e-05]]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "[1, -1, -1] [ 1 -1 -1]\n",
      "Accuracy:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Load JSON input\n",
    "with open(\"test/backpropagation/iris.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract data\n",
    "model = data[\"case\"][\"model\"]\n",
    "initial_weights = [np.array(layer_weights) for layer_weights in data[\"case\"][\"initial_weights\"]]\n",
    "learning_rate = data[\"case\"][\"learning_parameters\"][\"learning_rate\"]\n",
    "batch_size = data[\"case\"][\"learning_parameters\"][\"batch_size\"]\n",
    "max_iteration = data[\"case\"][\"learning_parameters\"][\"max_iteration\"]\n",
    "error_threshold = data[\"case\"][\"learning_parameters\"][\"error_threshold\"]\n",
    "stopped_by = data[\"expect\"][\"stopped_by\"]\n",
    "final_weights = [np.array(layer_weights) for layer_weights in data[\"expect\"][\"final_weights\"]]\n",
    "\n",
    "# iris with implemented code\n",
    "input_data = learning_data.to_numpy()\n",
    "target = []\n",
    "for i in range(len(target_data)):\n",
    "    if target_data[i] == \"Iris-setosa\":\n",
    "        target.append([1,-1,-1])\n",
    "    elif target_data[i] == \"Iris-versicolor\":\n",
    "        target.append([-1,1,-1])\n",
    "    else:\n",
    "        target.append([-1,-1,1])\n",
    "target = np.array(target)\n",
    "\n",
    "new_weight = back_propagation(model, input_data, initial_weights, target, learning_rate, batch_size, stopped_by, max_iteration, error_threshold)\n",
    "print(\"new weight: \", new_weight)\n",
    "output = forward_propagation(model, input_data, new_weight)\n",
    "print(\"output: \", output[-1])\n",
    "# print(output[-1])\n",
    "# print(target)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(output[-1])):\n",
    "    max = -99000\n",
    "    for j in range(3):\n",
    "        if output[-1][i][j] > max:\n",
    "            max = output[-1][i][j]\n",
    "            idx = j\n",
    "    \n",
    "    if (idx == 0) : \n",
    "        temp = [1,-1,-1]\n",
    "    elif (idx == 1):\n",
    "        temp = [-1,1,-1]\n",
    "    else:\n",
    "        temp = [-1,-1,1]\n",
    "\n",
    "    if (np.array_equal(temp, target[i])) :\n",
    "        print(temp,target[i])\n",
    "        count += 1\n",
    "print(\"Accuracy: \", count/len(output[-1]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
